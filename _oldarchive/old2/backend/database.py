"""
YouTube Analyzer - Database Manager
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SQLite –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ö–µ–º–æ–π
"""
import sqlite3
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Tuple
from pathlib import Path
import sys

# –î–æ–±–∞–≤–∏—Ç—å –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.append(str(project_root))

from shared.config import settings

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or settings.DATABASE_PATH
        self.init_database()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –í–∫–ª—é—á–∏—Ç—å WAL —Ä–µ–∂–∏–º –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            cursor.execute("PRAGMA journal_mode = WAL")
            cursor.execute("PRAGMA synchronous = NORMAL")
            cursor.execute("PRAGMA cache_size = 1000000")
            cursor.execute("PRAGMA temp_store = memory")
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤–∏–¥–µ–æ
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS videos (
                    video_id TEXT PRIMARY KEY,
                    channel_id TEXT NOT NULL,
                    channel_title TEXT,
                    title TEXT NOT NULL,
                    description TEXT,
                    duration TEXT,
                    duration_seconds INTEGER,
                    is_short BOOLEAN DEFAULT FALSE,
                    views INTEGER DEFAULT 0,
                    likes INTEGER DEFAULT 0,
                    comments INTEGER DEFAULT 0,
                    publish_date TEXT,
                    thumbnail_url TEXT,
                    has_cc BOOLEAN DEFAULT FALSE,
                    video_quality TEXT DEFAULT 'sd',
                    tags TEXT, -- JSON array
                    category_id TEXT,
                    transcript TEXT,
                    transcript_source TEXT,
                    keywords TEXT, -- JSON array
                    has_branding BOOLEAN DEFAULT FALSE,
                    speech_speed REAL,
                    has_intro BOOLEAN DEFAULT FALSE,
                    has_outro BOOLEAN DEFAULT FALSE,
                    like_ratio REAL,
                    comment_ratio REAL,
                    engagement_rate REAL,
                    recommendations TEXT,
                    success_analysis TEXT,
                    task_id TEXT,
                    analyzed_at TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    
                    -- –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    dislikes INTEGER DEFAULT 0,
                    average_view_duration REAL,
                    click_through_rate REAL,
                    has_pinned_comment BOOLEAN DEFAULT FALSE,
                    links_in_description INTEGER DEFAULT 0,
                    links_in_channel_description INTEGER DEFAULT 0,
                    contacts_in_video TEXT, -- JSON array
                    contacts_in_channel TEXT, -- JSON array
                    top_5_keywords TEXT, -- JSON array
                    video_category TEXT,
                    has_chapters BOOLEAN DEFAULT FALSE,
                    emoji_in_title BOOLEAN DEFAULT FALSE,
                    title_length INTEGER,
                    description_length INTEGER,
                    hashtags_count INTEGER DEFAULT 0,
                    mentioned_channels TEXT, -- JSON array
                    video_language TEXT,
                    is_live_stream BOOLEAN DEFAULT FALSE,
                    is_premiere BOOLEAN DEFAULT FALSE,
                    has_cards BOOLEAN DEFAULT FALSE,
                    has_end_screen BOOLEAN DEFAULT FALSE,
                    upload_schedule_consistency REAL,
                    competitor_comparison TEXT, -- JSON object
                    improvement_suggestions TEXT, -- JSON array
                    content_strategy TEXT,
                    estimated_revenue REAL,
                    audience_retention_score REAL,
                    seo_score INTEGER,
                    thumbnail_click_score INTEGER,
                    
                    FOREIGN KEY (channel_id) REFERENCES channels (channel_id)
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –∫–∞–Ω–∞–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS channels (
                    channel_id TEXT PRIMARY KEY,
                    channel_title TEXT NOT NULL,
                    channel_url TEXT,
                    subscriber_count INTEGER DEFAULT 0,
                    video_count INTEGER DEFAULT 0,
                    view_count INTEGER DEFAULT 0,
                    created_at TEXT,
                    avg_views REAL,
                    avg_likes REAL,
                    last_updated TEXT DEFAULT CURRENT_TIMESTAMP,
                    
                    -- –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    channel_description TEXT,
                    channel_keywords TEXT, -- JSON array
                    country TEXT,
                    custom_url TEXT,
                    default_language TEXT,
                    upload_frequency REAL, -- videos per month
                    avg_video_duration REAL,
                    avg_engagement_rate REAL,
                    total_engagement INTEGER,
                    channel_type TEXT, -- personal, brand, media
                    monetization_enabled BOOLEAN DEFAULT TRUE,
                    verified BOOLEAN DEFAULT FALSE,
                    family_safe BOOLEAN DEFAULT TRUE,
                    topics TEXT, -- JSON array
                    social_links TEXT, -- JSON object
                    email_in_description TEXT,
                    business_email TEXT,
                    has_channel_trailer BOOLEAN DEFAULT FALSE,
                    playlists_count INTEGER DEFAULT 0,
                    community_tab_enabled BOOLEAN DEFAULT FALSE,
                    merchandise_shelf_enabled BOOLEAN DEFAULT FALSE,
                    memberships_enabled BOOLEAN DEFAULT FALSE,
                    super_chat_enabled BOOLEAN DEFAULT FALSE,
                    competitor_channels TEXT, -- JSON array
                    growth_rate REAL,
                    estimated_monthly_revenue REAL,
                    brand_partnerships TEXT -- JSON array
                )
            ''')
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∑–∞–¥–∞—á —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS tasks (
                    task_id TEXT PRIMARY KEY,
                    task_type TEXT DEFAULT 'analysis',
                    status TEXT DEFAULT 'created',
                    progress INTEGER DEFAULT 0,
                    total_items INTEGER DEFAULT 0,
                    config TEXT, -- JSON
                    error_message TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    
                    -- –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
                    checkpoint_data TEXT, -- JSON with recovery info
                    last_checkpoint_at TEXT,
                    processing_time INTEGER DEFAULT 0, -- seconds
                    items_processed TEXT, -- JSON array of processed IDs
                    items_failed TEXT, -- JSON array of failed IDs
                    retry_count INTEGER DEFAULT 0,
                    paused_at TEXT,
                    resumed_at TEXT,
                    completed_at TEXT,
                    estimated_completion TEXT,
                    resources_used TEXT -- JSON object with API calls, etc
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—Ä–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS parsing_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id TEXT,
                    channel_id TEXT,
                    action TEXT,
                    status TEXT,
                    error_message TEXT,
                    task_id TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    
                    -- –ù–æ–≤—ã–µ –ø–æ–ª—è
                    retry_count INTEGER DEFAULT 0,
                    processing_time REAL,
                    api_calls_used INTEGER DEFAULT 0,
                    data_source TEXT, -- api, cache, manual
                    metadata TEXT -- JSON with additional info
                )
            ''')
            
            # –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS transcript_cache (
                    video_id TEXT PRIMARY KEY,
                    transcript TEXT,
                    language TEXT,
                    source TEXT, -- youtube, whisper, manual
                    confidence_score REAL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    expires_at TEXT
                )
            ''')
            
            # –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trend_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    keyword TEXT,
                    date TEXT,
                    video_count INTEGER,
                    avg_views INTEGER,
                    avg_engagement REAL,
                    top_channels TEXT, -- JSON array
                    trending_topics TEXT, -- JSON array
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS competitor_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    channel_id TEXT,
                    competitor_channel_id TEXT,
                    comparison_date TEXT,
                    metrics_comparison TEXT, -- JSON object
                    strengths TEXT, -- JSON array
                    weaknesses TEXT, -- JSON array
                    opportunities TEXT, -- JSON array
                    recommendations TEXT, -- JSON array
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_channel_id ON videos(channel_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_task_id ON videos(task_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_analyzed_at ON videos(analyzed_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_is_short ON videos(is_short)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_engagement ON videos(engagement_rate)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_views ON videos(views)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_videos_publish_date ON videos(publish_date)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_tasks_created_at ON tasks(created_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_channels_subscriber_count ON channels(subscriber_count)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_transcript_cache_expires ON transcript_cache(expires_at)')
            
            conn.commit()
            conn.close()
            print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ö–µ–º–æ–π")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
            raise
    
    def get_connection(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row  # –î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–ª–æ–Ω–∫–∞–º –ø–æ –∏–º–µ–Ω–∏
        return conn
    
    # ============ VIDEOS ============
    
    def video_exists(self, video_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ"""
        conn = self.get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM videos WHERE video_id = ?", (video_id,))
        exists = cursor.fetchone() is not None
        conn.close()
        return exists
    
    def save_video(self, video_data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–∏–¥–µ–æ –≤ –ë–î"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –≤ JSON
            json_fields = ['tags', 'keywords', 'top_5_keywords', 'contacts_in_video', 
                          'contacts_in_channel', 'mentioned_channels', 'improvement_suggestions']
            
            for field in json_fields:
                if field in video_data and isinstance(video_data[field], list):
                    video_data[field] = json.dumps(video_data[field], ensure_ascii=False)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ JSON
            object_fields = ['competitor_comparison']
            for field in object_fields:
                if field in video_data and isinstance(video_data[field], dict):
                    video_data[field] = json.dumps(video_data[field], ensure_ascii=False)
            
            # –î–æ–±–∞–≤–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            now = datetime.now().isoformat()
            video_data['updated_at'] = now
            if 'created_at' not in video_data:
                video_data['created_at'] = now
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            columns = list(video_data.keys())
            placeholders = ','.join(['?' for _ in columns])
            values = [video_data[col] for col in columns]
            
            cursor.execute(f'''
                INSERT OR REPLACE INTO videos ({','.join(columns)})
                VALUES ({placeholders})
            ''', values)
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–¥–µ–æ: {e}")
        finally:
            conn.close()
    
    def get_videos(self, limit: int = 100, offset: int = 0, filters: Dict = None) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤–∏–¥–µ–æ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        query = '''
            SELECT * FROM videos 
            WHERE 1=1
        '''
        params = []
        
        if filters:
            if filters.get('is_short') is not None:
                query += ' AND is_short = ?'
                params.append(filters['is_short'])
            
            if filters.get('channel_id'):
                query += ' AND channel_id = ?'
                params.append(filters['channel_id'])
            
            if filters.get('min_views'):
                query += ' AND views >= ?'
                params.append(filters['min_views'])
            
            if filters.get('min_engagement'):
                query += ' AND engagement_rate >= ?'
                params.append(filters['min_engagement'])
        
        query += '''
            ORDER BY analyzed_at DESC, created_at DESC
            LIMIT ? OFFSET ?
        '''
        params.extend([limit, offset])
        
        cursor.execute(query, params)
        
        videos = []
        for row in cursor.fetchall():
            video = dict(row)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            for field in ['tags', 'keywords', 'top_5_keywords', 'contacts_in_video', 
                         'contacts_in_channel', 'mentioned_channels', 'improvement_suggestions']:
                if video.get(field):
                    try:
                        video[field] = json.loads(video[field])
                    except:
                        video[field] = []
            
            for field in ['competitor_comparison']:
                if video.get(field):
                    try:
                        video[field] = json.loads(video[field])
                    except:
                        video[field] = {}
            
            videos.append(video)
        
        conn.close()
        return videos
    
    def get_video_by_id(self, video_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–∏–¥–µ–æ –ø–æ ID"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM videos WHERE video_id = ?", (video_id,))
        row = cursor.fetchone()
        
        if row:
            video = dict(row)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            for field in ['tags', 'keywords', 'top_5_keywords', 'contacts_in_video', 
                         'contacts_in_channel', 'mentioned_channels', 'improvement_suggestions']:
                if video.get(field):
                    try:
                        video[field] = json.loads(video[field])
                    except:
                        video[field] = []
            
            for field in ['competitor_comparison']:
                if video.get(field):
                    try:
                        video[field] = json.loads(video[field])
                    except:
                        video[field] = {}
            
            conn.close()
            return video
        
        conn.close()
        return None
    
    # ============ CHANNELS ============
    
    def save_channel(self, channel_data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–Ω–∞–ª –≤ –ë–î"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            json_fields = ['channel_keywords', 'topics', 'competitor_channels', 'brand_partnerships']
            for field in json_fields:
                if field in channel_data and isinstance(channel_data[field], list):
                    channel_data[field] = json.dumps(channel_data[field], ensure_ascii=False)
            
            if 'social_links' in channel_data and isinstance(channel_data['social_links'], dict):
                channel_data['social_links'] = json.dumps(channel_data['social_links'], ensure_ascii=False)
            
            # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª–∞
            cursor.execute('''
                SELECT 
                    AVG(views) as avg_views, 
                    AVG(likes) as avg_likes,
                    AVG(engagement_rate) as avg_engagement_rate,
                    AVG(duration_seconds) as avg_video_duration,
                    SUM(views + likes + comments) as total_engagement
                FROM videos WHERE channel_id = ?
            ''', (channel_data['channel_id'],))
            
            avgs = cursor.fetchone()
            if avgs and avgs[0]:
                channel_data['avg_views'] = avgs[0]
                channel_data['avg_likes'] = avgs[1]
                channel_data['avg_engagement_rate'] = avgs[2] or 0
                channel_data['avg_video_duration'] = avgs[3] or 0
                channel_data['total_engagement'] = avgs[4] or 0
            
            # –û–±–Ω–æ–≤–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
            channel_data['last_updated'] = datetime.now().isoformat()
            
            columns = list(channel_data.keys())
            placeholders = ','.join(['?' for _ in columns])
            values = [channel_data[col] for col in columns]
            
            cursor.execute(f'''
                INSERT OR REPLACE INTO channels ({','.join(columns)})
                VALUES ({placeholders})
            ''', values)
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–Ω–∞–ª–∞: {e}")
        finally:
            conn.close()
    
    def get_channels(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                c.*,
                COUNT(v.video_id) as analyzed_videos
            FROM channels c
            LEFT JOIN videos v ON c.channel_id = v.channel_id
            GROUP BY c.channel_id
            ORDER BY c.subscriber_count DESC, c.last_updated DESC
        ''')
        
        channels = []
        for row in cursor.fetchall():
            channel = dict(row)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            for field in ['channel_keywords', 'topics', 'competitor_channels', 'brand_partnerships']:
                if channel.get(field):
                    try:
                        channel[field] = json.loads(channel[field])
                    except:
                        channel[field] = []
            
            if channel.get('social_links'):
                try:
                    channel['social_links'] = json.loads(channel['social_links'])
                except:
                    channel['social_links'] = {}
            
            channels.append(channel)
        
        conn.close()
        return channels
    
    def get_channel_stats(self, channel_id: str) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–∞"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞–Ω–∞–ª–µ
        cursor.execute("SELECT * FROM channels WHERE channel_id = ?", (channel_id,))
        channel_row = cursor.fetchone()
        
        if channel_row:
            channel_info = dict(channel_row)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            for field in ['channel_keywords', 'topics', 'competitor_channels', 'brand_partnerships']:
                if channel_info.get(field):
                    try:
                        channel_info[field] = json.loads(channel_info[field])
                    except:
                        channel_info[field] = []
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª–∞
            cursor.execute('''
                SELECT 
                    AVG(views) as avg_views,
                    AVG(likes) as avg_likes,
                    AVG(comments) as avg_comments,
                    AVG(engagement_rate) as avg_engagement,
                    COUNT(*) as video_count,
                    COUNT(CASE WHEN is_short = 1 THEN 1 END) as shorts_count,
                    COUNT(CASE WHEN is_short = 0 THEN 1 END) as videos_count,
                    MAX(views) as max_views,
                    MIN(views) as min_views,
                    AVG(CASE WHEN is_short = 1 THEN engagement_rate END) as shorts_engagement,
                    AVG(CASE WHEN is_short = 0 THEN engagement_rate END) as videos_engagement
                FROM videos 
                WHERE channel_id = ?
            ''', (channel_id,))
            
            stats = cursor.fetchone()
            if stats:
                channel_info.update(dict(stats))
            
            # –¢–æ–ø –≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª–∞
            cursor.execute('''
                SELECT video_id, title, views, engagement_rate
                FROM videos
                WHERE channel_id = ?
                ORDER BY views DESC
                LIMIT 5
            ''', (channel_id,))
            
            channel_info['top_videos'] = [dict(row) for row in cursor.fetchall()]
            
            conn.close()
            return channel_info
        
        conn.close()
        return {}
    
    # ============ TASKS ============
    
    def save_task(self, task_data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–¥–∞—á—É"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ JSON
            for field in ['config', 'checkpoint_data', 'items_processed', 'items_failed', 'resources_used']:
                if field in task_data and isinstance(task_data[field], (dict, list)):
                    task_data[field] = json.dumps(task_data[field], ensure_ascii=False)
            
            columns = list(task_data.keys())
            placeholders = ','.join(['?' for _ in columns])
            values = [task_data[col] for col in columns]
            
            cursor.execute(f'''
                INSERT OR REPLACE INTO tasks ({','.join(columns)})
                VALUES ({placeholders})
            ''', values)
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏: {e}")
        finally:
            conn.close()
    
    def get_all_tasks(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∑–∞–¥–∞—á–∏"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM tasks 
            ORDER BY created_at DESC
            LIMIT 100
        ''')
        
        tasks = []
        for row in cursor.fetchall():
            task = dict(row)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            for field in ['config', 'checkpoint_data', 'items_processed', 'items_failed', 'resources_used']:
                if task.get(field):
                    try:
                        task[field] = json.loads(task[field])
                    except:
                        task[field] = {} if field in ['config', 'checkpoint_data', 'resources_used'] else []
            tasks.append(task)
        
        conn.close()
        return tasks
    
    def get_task(self, task_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–¥–∞—á—É –ø–æ ID"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
        row = cursor.fetchone()
        
        if row:
            task = dict(row)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –ø–æ–ª–µ–π
            for field in ['config', 'checkpoint_data', 'items_processed', 'items_failed', 'resources_used']:
                if task.get(field):
                    try:
                        task[field] = json.loads(task[field])
                    except:
                        task[field] = {} if field in ['config', 'checkpoint_data', 'resources_used'] else []
            conn.close()
            return task
        
        conn.close()
        return None
    
    def update_task_status(self, task_id: str, status: str, error_message: str = None):
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        update_fields = {
            'status': status,
            'updated_at': now
        }
        
        if error_message:
            update_fields['error_message'] = error_message
        
        if status == 'paused':
            update_fields['paused_at'] = now
        elif status == 'running' and cursor.execute("SELECT paused_at FROM tasks WHERE task_id = ?", (task_id,)).fetchone():
            update_fields['resumed_at'] = now
        elif status == 'completed':
            update_fields['completed_at'] = now
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        set_clause = ', '.join([f"{k} = ?" for k in update_fields.keys()])
        values = list(update_fields.values()) + [task_id]
        
        cursor.execute(f'''
            UPDATE tasks 
            SET {set_clause}
            WHERE task_id = ?
        ''', values)
        
        conn.commit()
        conn.close()
    
    def update_task_progress(self, task_id: str, progress: int, total: int):
        """–û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–¥–∞—á–∏"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE tasks 
            SET progress = ?, total_items = ?, updated_at = ?
            WHERE task_id = ?
        ''', (progress, total, datetime.now().isoformat(), task_id))
        
        conn.commit()
        conn.close()
    
    # ============ TASK CHECKPOINT METHODS ============
    
    def save_task_checkpoint(self, task_id: str, checkpoint_data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–¥–∞—á–∏ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            checkpoint_json = json.dumps(checkpoint_data, ensure_ascii=False)
            now = datetime.now().isoformat()
            
            cursor.execute('''
                UPDATE tasks 
                SET checkpoint_data = ?, last_checkpoint_at = ?, updated_at = ?
                WHERE task_id = ?
            ''', (checkpoint_json, now, now, task_id))
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
        finally:
            conn.close()
    
    def get_task_checkpoint(self, task_id: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–¥–∞—á–∏"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT checkpoint_data, last_checkpoint_at 
            FROM tasks 
            WHERE task_id = ?
        ''', (task_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row and row['checkpoint_data']:
            try:
                checkpoint = json.loads(row['checkpoint_data'])
                checkpoint['last_checkpoint_at'] = row['last_checkpoint_at']
                return checkpoint
            except json.JSONDecodeError:
                return None
        return None
    
    def update_task_progress_extended(self, task_id: str, progress: int, total: int, 
                                    processed_items: List[str] = None, 
                                    failed_items: List[str] = None):
        """–û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–¥–∞—á–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            cursor.execute('''
                SELECT items_processed, items_failed, processing_time, created_at
                FROM tasks WHERE task_id = ?
            ''', (task_id,))
            
            current = cursor.fetchone()
            
            # –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if current:
                current_processed = json.loads(current['items_processed'] or '[]')
                current_failed = json.loads(current['items_failed'] or '[]')
                
                if processed_items:
                    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å set –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                    current_processed = list(set(current_processed + processed_items))
                if failed_items:
                    current_failed = list(set(current_failed + failed_items))
                
                # –í—ã—á–∏—Å–ª–∏—Ç—å –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                start_time = datetime.fromisoformat(current['created_at'])
                processing_time = int((datetime.now() - start_time).total_seconds())
                
                # –û—Ü–µ–Ω–∏—Ç—å –≤—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if progress > 0 and progress < total:
                    time_per_item = processing_time / progress
                    remaining_items = total - progress
                    estimated_seconds = remaining_items * time_per_item
                    estimated_completion = (datetime.now() + timedelta(seconds=estimated_seconds)).isoformat()
                else:
                    estimated_completion = None
                
                cursor.execute('''
                    UPDATE tasks 
                    SET progress = ?, total_items = ?, 
                        items_processed = ?, items_failed = ?,
                        processing_time = ?, estimated_completion = ?,
                        updated_at = ?
                    WHERE task_id = ?
                ''', (progress, total, 
                      json.dumps(current_processed, ensure_ascii=False),
                      json.dumps(current_failed, ensure_ascii=False),
                      processing_time, estimated_completion,
                      datetime.now().isoformat(), task_id))
                
                conn.commit()
                
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
        finally:
            conn.close()
    
    # ============ TRANSCRIPT CACHE METHODS ============
    
    def cache_transcript(self, video_id: str, transcript: str, language: str, 
                        source: str, confidence_score: float = None):
        """–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤–∏–¥–µ–æ"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å—Ä–æ–∫ –∏—Å—Ç–µ—á–µ–Ω–∏—è –∫—ç—à–∞ (30 –¥–Ω–µ–π)
            expires_at = (datetime.now() + timedelta(days=30)).isoformat()
            
            cursor.execute('''
                INSERT OR REPLACE INTO transcript_cache 
                (video_id, transcript, language, source, confidence_score, expires_at)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (video_id, transcript, language, source, confidence_score, expires_at))
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞: {e}")
        finally:
            conn.close()
    
    def get_cached_transcript(self, video_id: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT transcript, language, source, confidence_score
            FROM transcript_cache
            WHERE video_id = ? AND expires_at > ?
        ''', (video_id, datetime.now().isoformat()))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return {
                'transcript': row['transcript'],
                'language': row['language'],
                'source': row['source'],
                'confidence_score': row['confidence_score']
            }
        return None
    
    def clean_expired_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–µ–∫—à–∏–π –∫—ç—à"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            DELETE FROM transcript_cache
            WHERE expires_at < ?
        ''', (datetime.now().isoformat(),))
        
        deleted = cursor.rowcount
        conn.commit()
        conn.close()
        
        if deleted > 0:
            print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ {deleted} –∏—Å—Ç–µ–∫—à–∏—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –∏–∑ –∫—ç—à–∞")
    
    # ============ ANALYTICS METHODS ============
    
    def get_channel_upload_frequency(self, channel_id: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª–∞"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT publish_date
            FROM videos
            WHERE channel_id = ?
            ORDER BY publish_date DESC
        ''', (channel_id,))
        
        dates = [row['publish_date'] for row in cursor.fetchall()]
        conn.close()
        
        if len(dates) < 2:
            return 0.0
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∑–∫–∞–º–∏
        date_objects = [datetime.fromisoformat(d.replace('Z', '+00:00')) for d in dates if d]
        differences = []
        
        for i in range(1, len(date_objects)):
            diff = (date_objects[i-1] - date_objects[i]).days
            if diff > 0:
                differences.append(diff)
        
        if differences:
            avg_days = sum(differences) / len(differences)
            return 30 / avg_days if avg_days > 0 else 0.0  # Videos per month
        return 0.0
    
    def analyze_competitor_channels(self, channel_id: str, competitor_ids: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∫–∞–Ω–∞–ª–∞"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        analysis = {
            'channel_id': channel_id,
            'competitors': {},
            'recommendations': []
        }
        
        # –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        cursor.execute('''
            SELECT 
                AVG(views) as avg_views,
                AVG(engagement_rate) as avg_engagement,
                COUNT(*) as video_count,
                AVG(duration_seconds) as avg_duration
            FROM videos
            WHERE channel_id = ?
        ''', (channel_id,))
        
        main_metrics = dict(cursor.fetchone())
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
        for comp_id in competitor_ids:
            cursor.execute('''
                SELECT 
                    AVG(views) as avg_views,
                    AVG(engagement_rate) as avg_engagement,
                    COUNT(*) as video_count,
                    AVG(duration_seconds) as avg_duration
                FROM videos
                WHERE channel_id = ?
            ''', (comp_id,))
            
            comp_row = cursor.fetchone()
            if comp_row:
                comp_metrics = dict(comp_row)
                
                comparison = {
                    'avg_views_diff': (comp_metrics['avg_views'] - main_metrics['avg_views']) / main_metrics['avg_views'] * 100 if main_metrics['avg_views'] else 0,
                    'avg_engagement_diff': comp_metrics['avg_engagement'] - main_metrics['avg_engagement'],
                    'video_count': comp_metrics['video_count'],
                    'avg_duration': comp_metrics['avg_duration']
                }
                
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                if comparison['avg_views_diff'] > 50:
                    analysis['recommendations'].append(f"–ò–∑—É—á–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–∞–Ω–∞–ª–∞ {comp_id} - –∏—Ö –≤–∏–¥–µ–æ –ø–æ–ª—É—á–∞—é—Ç –Ω–∞ {comparison['avg_views_diff']:.0f}% –±–æ–ª—å—à–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤")
                
                if comparison['avg_engagement_diff'] > 2:
                    analysis['recommendations'].append(f"–ö–∞–Ω–∞–ª {comp_id} –∏–º–µ–µ—Ç –ª—É—á—à—É—é –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å (+{comparison['avg_engagement_diff']:.1f}%) - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏—Ö –ø–æ–¥—Ö–æ–¥")
                
                analysis['competitors'][comp_id] = comparison
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑
        self.save_competitor_analysis(channel_id, competitor_ids[0] if competitor_ids else None, analysis)
        
        conn.close()
        return analysis
    
    def save_competitor_analysis(self, channel_id: str, competitor_id: str, analysis: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO competitor_analysis 
                (channel_id, competitor_channel_id, comparison_date, 
                 metrics_comparison, recommendations)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                channel_id,
                competitor_id,
                datetime.now().date().isoformat(),
                json.dumps(analysis.get('competitors', {}), ensure_ascii=False),
                json.dumps(analysis.get('recommendations', []), ensure_ascii=False)
            ))
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        finally:
            conn.close()
    
    def save_trend_analysis(self, keyword: str, analysis_data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO trend_analysis 
                (keyword, date, video_count, avg_views, avg_engagement, 
                 top_channels, trending_topics)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                keyword,
                datetime.now().date().isoformat(),
                analysis_data.get('video_count', 0),
                analysis_data.get('avg_views', 0),
                analysis_data.get('avg_engagement', 0),
                json.dumps(analysis_data.get('top_channels', []), ensure_ascii=False),
                json.dumps(analysis_data.get('trending_topics', []), ensure_ascii=False)
            ))
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        finally:
            conn.close()
    
    # ============ STATISTICS ============
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        stats = {}
        
        # –û–±—â–∏–µ —Å—á–µ—Ç—á–∏–∫–∏
        cursor.execute("SELECT COUNT(*) FROM videos")
        stats['total_videos'] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM channels")
        stats['total_channels'] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM tasks")
        stats['total_tasks'] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM tasks WHERE status = 'completed'")
        stats['completed_tasks'] = cursor.fetchone()[0]
        
        # –°—Ä–µ–¥–Ω—è—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å
        cursor.execute("SELECT AVG(engagement_rate) FROM videos WHERE engagement_rate IS NOT NULL")
        avg_engagement = cursor.fetchone()[0]
        stats['avg_engagement'] = round(avg_engagement, 2) if avg_engagement else 0
        
        # –í–∏–¥–µ–æ –ø–æ —Ç–∏–ø–∞–º
        cursor.execute('''
            SELECT 
                CASE WHEN is_short THEN 'shorts' ELSE 'long' END as type,
                COUNT(*) as count
            FROM videos 
            GROUP BY is_short
        ''')
        
        videos_by_type = {}
        for row in cursor.fetchall():
            videos_by_type[row[0]] = row[1]
        stats['videos_by_type'] = videos_by_type
        
        # –¢–æ–ø –∫–∞–Ω–∞–ª—ã –ø–æ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
        cursor.execute('''
            SELECT 
                c.channel_title,
                c.subscriber_count,
                COUNT(v.video_id) as video_count,
                AVG(v.engagement_rate) as avg_engagement,
                SUM(v.views) as total_views
            FROM channels c
            LEFT JOIN videos v ON c.channel_id = v.channel_id
            GROUP BY c.channel_id
            HAVING video_count > 0
            ORDER BY avg_engagement DESC
            LIMIT 10
        ''')
        
        top_channels = []
        for row in cursor.fetchall():
            top_channels.append({
                'channel_title': row[0],
                'subscriber_count': row[1] or 0,
                'video_count': row[2],
                'avg_engagement': round(row[3], 2) if row[3] else 0,
                'total_views': row[4] or 0
            })
        stats['top_channels'] = top_channels
        
        # –¢–æ–ø –≤–∏–¥–µ–æ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º
        cursor.execute('''
            SELECT 
                title,
                channel_title,
                views,
                engagement_rate,
                video_id
            FROM videos
            ORDER BY views DESC
            LIMIT 10
        ''')
        
        top_videos = []
        for row in cursor.fetchall():
            top_videos.append({
                'title': row[0],
                'channel_title': row[1],
                'views': row[2],
                'engagement_rate': round(row[3], 2) if row[3] else 0,
                'video_id': row[4]
            })
        stats['top_videos'] = top_videos
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º
        cursor.execute('''
            SELECT 
                status,
                COUNT(*) as count,
                AVG(processing_time) / 60.0 as avg_minutes
            FROM tasks
            GROUP BY status
        ''')
        
        task_stats = {}
        for row in cursor.fetchall():
            task_stats[row[0]] = {
                'count': row[1],
                'avg_minutes': round(row[2], 1) if row[2] else 0
            }
        stats['task_stats'] = task_stats
        
        conn.close()
        return stats
    
    # ============ ENHANCED EXPORT ============
    
    def export_to_excel(self, filename: str = None) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ Excel —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –ª–∏—Å—Ç–æ–≤"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"youtube_analysis_{timestamp}.xlsx"
        
        export_path = Path(settings.EXPORT_DIR) / filename
        export_path.parent.mkdir(exist_ok=True)
        
        conn = self.get_connection()
        
        try:
            with pd.ExcelWriter(export_path, engine='openpyxl') as writer:
                # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ
                videos_query = '''
                    SELECT 
                        'https://youtube.com/watch?v=' || video_id as video_url,
                        title, channel_title, 
                        CASE WHEN is_short THEN 'SHORT' ELSE 'VIDEO' END as type,
                        views, likes, comments,
                        engagement_rate, like_ratio, comment_ratio,
                        duration, publish_date,
                        has_cc, video_quality, has_branding,
                        speech_speed, emoji_in_title,
                        links_in_description, hashtags_count,
                        analyzed_at
                    FROM videos
                    ORDER BY analyzed_at DESC
                '''
                videos_df = pd.read_sql_query(videos_query, conn)
                videos_df.to_excel(writer, sheet_name='Videos', index=False)
                
                # 2. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –≤–∏–¥–µ–æ
                analytics_query = '''
                    SELECT 
                        video_id, title,
                        average_view_duration, click_through_rate,
                        has_chapters, has_intro, has_outro,
                        has_pinned_comment, has_cards, has_end_screen,
                        seo_score, thumbnail_click_score,
                        audience_retention_score, estimated_revenue,
                        top_5_keywords, recommendations,
                        success_analysis, content_strategy,
                        improvement_suggestions
                    FROM videos
                    WHERE analyzed_at IS NOT NULL
                    ORDER BY engagement_rate DESC
                '''
                analytics_df = pd.read_sql_query(analytics_query, conn)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å JSON –ø–æ–ª—è
                if not analytics_df.empty:
                    for json_field in ['top_5_keywords', 'improvement_suggestions']:
                        analytics_df[json_field] = analytics_df[json_field].apply(
                            lambda x: ', '.join(json.loads(x)) if x and x != 'null' and x != '[]' else ''
                        )
                
                analytics_df.to_excel(writer, sheet_name='Video Analytics', index=False)
                
                # 3. –î–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª–æ–≤
                channels_query = '''
                    SELECT 
                        channel_title, channel_url,
                        subscriber_count, video_count, view_count,
                        avg_views, avg_engagement_rate,
                        upload_frequency, channel_type,
                        verified, monetization_enabled,
                        growth_rate, estimated_monthly_revenue
                    FROM channels
                    ORDER BY subscriber_count DESC
                '''
                channels_df = pd.read_sql_query(channels_query, conn)
                channels_df.to_excel(writer, sheet_name='Channels', index=False)
                
                # 4. –¢–æ–ø –≤–∏–¥–µ–æ
                top_videos_query = '''
                    SELECT 
                        title, channel_title, views, engagement_rate,
                        'https://youtube.com/watch?v=' || video_id as url
                    FROM videos
                    ORDER BY views DESC
                    LIMIT 50
                '''
                top_videos_df = pd.read_sql_query(top_videos_query, conn)
                top_videos_df.to_excel(writer, sheet_name='Top 50 Videos', index=False)
                
                # 5. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
                trends_query = '''
                    SELECT 
                        keyword, date, video_count,
                        avg_views, avg_engagement,
                        trending_topics
                    FROM trend_analysis
                    ORDER BY date DESC, avg_views DESC
                '''
                trends_df = pd.read_sql_query(trends_query, conn)
                if not trends_df.empty:
                    trends_df['trending_topics'] = trends_df['trending_topics'].apply(
                        lambda x: ', '.join(json.loads(x)) if x and x != 'null' else ''
                    )
                    trends_df.to_excel(writer, sheet_name='Trends', index=False)
                
                # 6. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º
                tasks_query = '''
                    SELECT 
                        task_id, status, progress, total_items,
                        processing_time / 60.0 as processing_minutes,
                        created_at, completed_at
                    FROM tasks
                    ORDER BY created_at DESC
                '''
                tasks_df = pd.read_sql_query(tasks_query, conn)
                tasks_df.to_excel(writer, sheet_name='Tasks', index=False)
                
                # 7. –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                stats = self.get_statistics()
                summary_data = []
                
                summary_data.append({
                    '–ú–µ—Ç—Ä–∏–∫–∞': '–í—Å–µ–≥–æ –≤–∏–¥–µ–æ',
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': stats['total_videos']
                })
                summary_data.append({
                    '–ú–µ—Ç—Ä–∏–∫–∞': '–í—Å–µ–≥–æ –∫–∞–Ω–∞–ª–æ–≤',
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': stats['total_channels']
                })
                summary_data.append({
                    '–ú–µ—Ç—Ä–∏–∫–∞': '–°—Ä–µ–¥–Ω—è—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å',
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': f"{stats['avg_engagement']}%"
                })
                summary_data.append({
                    '–ú–µ—Ç—Ä–∏–∫–∞': 'Shorts –≤–∏–¥–µ–æ',
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': stats['videos_by_type'].get('shorts', 0)
                })
                summary_data.append({
                    '–ú–µ—Ç—Ä–∏–∫–∞': '–û–±—ã—á–Ω—ã—Ö –≤–∏–¥–µ–æ',
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': stats['videos_by_type'].get('long', 0)
                })
                summary_data.append({
                    '–ú–µ—Ç—Ä–∏–∫–∞': '–í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–¥–∞—á',
                    '–ó–Ω–∞—á–µ–Ω–∏–µ': stats['completed_tasks']
                })
                
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            conn.close()
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")
            return str(export_path)
            
        except Exception as e:
            conn.close()
            raise Exception(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
    
    # ============ MAINTENANCE ============
    
    def clear_database(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            tables = [
                'parsing_history', 'competitor_analysis', 'trend_analysis',
                'transcript_cache', 'videos', 'channels', 'tasks'
            ]
            
            for table in tables:
                cursor.execute(f"DELETE FROM {table}")
            
            # –°–±—Ä–æ—Å–∏—Ç—å —Å—á–µ—Ç—á–∏–∫–∏ –∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞
            cursor.execute("DELETE FROM sqlite_sequence")
            
            conn.commit()
            print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–µ–Ω–∞")
            
        except Exception as e:
            conn.rollback()
            raise Exception(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ë–î: {e}")
        finally:
            conn.close()
    
    def vacuum_database(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("VACUUM")
        conn.close()
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def backup_database(self, backup_path: str = None):
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ë–î"""
        import shutil
        
        if not backup_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"backups/youtube_data_backup_{timestamp}.db"
        
        backup_dir = Path(backup_path).parent
        backup_dir.mkdir(exist_ok=True)
        
        shutil.copy2(self.db_path, backup_path)
        print(f"‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_path}")
        return backup_path
    
    # ============ MIGRATION METHODS ============
    
    def migrate_database(self):
        """–ú–∏–≥—Ä–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∫ –Ω–æ–≤–æ–π —Å—Ö–µ–º–µ"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ videos
            cursor.execute("PRAGMA table_info(videos)")
            existing_columns = {col[1] for col in cursor.fetchall()}
            
            # –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ videos
            new_columns = {
                'dislikes': 'INTEGER DEFAULT 0',
                'average_view_duration': 'REAL',
                'click_through_rate': 'REAL',
                'has_pinned_comment': 'BOOLEAN DEFAULT FALSE',
                'links_in_description': 'INTEGER DEFAULT 0',
                'links_in_channel_description': 'INTEGER DEFAULT 0',
                'contacts_in_video': 'TEXT',
                'contacts_in_channel': 'TEXT',
                'top_5_keywords': 'TEXT',
                'video_category': 'TEXT',
                'has_chapters': 'BOOLEAN DEFAULT FALSE',
                'emoji_in_title': 'BOOLEAN DEFAULT FALSE',
                'title_length': 'INTEGER',
                'description_length': 'INTEGER',
                'hashtags_count': 'INTEGER DEFAULT 0',
                'mentioned_channels': 'TEXT',
                'video_language': 'TEXT',
                'is_live_stream': 'BOOLEAN DEFAULT FALSE',
                'is_premiere': 'BOOLEAN DEFAULT FALSE',
                'has_cards': 'BOOLEAN DEFAULT FALSE',
                'has_end_screen': 'BOOLEAN DEFAULT FALSE',
                'upload_schedule_consistency': 'REAL',
                'competitor_comparison': 'TEXT',
                'improvement_suggestions': 'TEXT',
                'content_strategy': 'TEXT',
                'estimated_revenue': 'REAL',
                'audience_retention_score': 'REAL',
                'seo_score': 'INTEGER',
                'thumbnail_click_score': 'INTEGER'
            }
            
            # –î–æ–±–∞–≤–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            for column, definition in new_columns.items():
                if column not in existing_columns:
                    try:
                        cursor.execute(f'ALTER TABLE videos ADD COLUMN {column} {definition}')
                        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {column}")
                    except sqlite3.OperationalError:
                        pass
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ channels
            cursor.execute("PRAGMA table_info(channels)")
            existing_channel_columns = {col[1] for col in cursor.fetchall()}
            
            new_channel_columns = {
                'channel_description': 'TEXT',
                'channel_keywords': 'TEXT',
                'country': 'TEXT',
                'custom_url': 'TEXT',
                'default_language': 'TEXT',
                'upload_frequency': 'REAL',
                'avg_video_duration': 'REAL',
                'avg_engagement_rate': 'REAL',
                'total_engagement': 'INTEGER',
                'channel_type': 'TEXT',
                'monetization_enabled': 'BOOLEAN DEFAULT TRUE',
                'verified': 'BOOLEAN DEFAULT FALSE',
                'family_safe': 'BOOLEAN DEFAULT TRUE',
                'topics': 'TEXT',
                'social_links': 'TEXT',
                'email_in_description': 'TEXT',
                'business_email': 'TEXT',
                'has_channel_trailer': 'BOOLEAN DEFAULT FALSE',
                'playlists_count': 'INTEGER DEFAULT 0',
                'community_tab_enabled': 'BOOLEAN DEFAULT FALSE',
                'merchandise_shelf_enabled': 'BOOLEAN DEFAULT FALSE',
                'memberships_enabled': 'BOOLEAN DEFAULT FALSE',
                'super_chat_enabled': 'BOOLEAN DEFAULT FALSE',
                'competitor_channels': 'TEXT',
                'growth_rate': 'REAL',
                'estimated_monthly_revenue': 'REAL',
                'brand_partnerships': 'TEXT'
            }
            
            for column, definition in new_channel_columns.items():
                if column not in existing_channel_columns:
                    try:
                        cursor.execute(f'ALTER TABLE channels ADD COLUMN {column} {definition}')
                        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –≤ channels: {column}")
                    except sqlite3.OperationalError:
                        pass
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ tasks
            cursor.execute("PRAGMA table_info(tasks)")
            existing_task_columns = {col[1] for col in cursor.fetchall()}
            
            new_task_columns = {
                'checkpoint_data': 'TEXT',
                'last_checkpoint_at': 'TEXT',
                'processing_time': 'INTEGER DEFAULT 0',
                'items_processed': 'TEXT',
                'items_failed': 'TEXT',
                'retry_count': 'INTEGER DEFAULT 0',
                'paused_at': 'TEXT',
                'resumed_at': 'TEXT',
                'completed_at': 'TEXT',
                'estimated_completion': 'TEXT',
                'resources_used': 'TEXT'
            }
            
            for column, definition in new_task_columns.items():
                if column not in existing_task_columns:
                    try:
                        cursor.execute(f'ALTER TABLE tasks ADD COLUMN {column} {definition}')
                        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –≤ tasks: {column}")
                    except sqlite3.OperationalError:
                        pass
            
            conn.commit()
            print("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
        except Exception as e:
            conn.rollback()
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")
            raise
        finally:
            conn.close()

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–∏–≥—Ä–∞—Ü–∏—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
if __name__ == "__main__":
    db = DatabaseManager()
    db.migrate_database()

print("‚úÖ Enhanced Database Manager –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")